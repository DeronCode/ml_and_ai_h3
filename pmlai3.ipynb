{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to Dataset: https://www.kaggle.com/datasets/uciml/glass\n",
    "\n",
    "In hindsight, the dataset I picked was too small and this didn't help with accuracy and training. The dataset was also skewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1550,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"glass.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1551,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(data,batch_size,bool1 = True):\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        chunk = (data[i:i + batch_size])\n",
    "        if bool1:\n",
    "            chunk = chunk.copy(deep = True)\n",
    "            y_chunk = chunk['Type']\n",
    "            y_chunk = y_chunk.to_numpy()\n",
    "            y_chunk = y_chunk.reshape(y_chunk.shape[0],1)\n",
    "            chunk.drop('Type', axis = 1, inplace = True)\n",
    "            chunk = chunk.to_numpy()\n",
    "            y_train.append(y_chunk)\n",
    "        x_train.append(chunk)\n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1552,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = get_batches(df,10,True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before implementing one hot and data normalization, I was getting an overflow on my softmax function due to the values being too large.\n",
    "This would cause me to divide by zero when it became NaN."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1554,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(Y):\n",
    "    one_hot_Y = np.zeros((Y.shape[1], 8))\n",
    "    one_hot_Y[np.arange(Y.shape[1]), Y] = 1\n",
    "    return one_hot_Y.T\n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.w1 = np.random.rand(64, 9) + .5\n",
    "        self.b1 = np.random.rand(64,1) + .5\n",
    "        self.w2 = np.random.rand(8,64) + .5\n",
    "        self.b2 = np.random.rand(8,1) + .5\n",
    "    def __str__(self):\n",
    "        return\n",
    "    def ReLU(self,Z):\n",
    "        return np.maximum(Z,0)\n",
    "    def d_reLU(self,Z):\n",
    "        return Z > 0 \n",
    "    def forward_propogation(self,X):\n",
    "        Z1 = self.w1.dot(X) + self.b1\n",
    "        A1 = self.ReLU(Z1)\n",
    "        Z2 = self.w2.dot(A1) + self.b2\n",
    "        A2 = self.softmax(Z2)\n",
    "        return Z1,A1,Z2,A2\n",
    "    def softmax(self,Z):\n",
    "        A = np.exp(Z) / sum(np.exp(Z))\n",
    "        return A\n",
    "\n",
    "    def mse_loss(self, Y, Y_pred):\n",
    "        return np.mean((Y - Y_pred) ** 2)\n",
    "    def backward_propogation(self,A1,Z1,A2,Z2,X,Y):\n",
    "        m = Y.shape[1]\n",
    "        one_hot_Y = one_hot(Y)\n",
    "        dZ2 = A2 - one_hot_Y\n",
    "        dW2 = 1/m * np.dot(dZ2, A1.T) \n",
    "        dB2 = 1/m * np.sum(dZ2,keepdims= True)\n",
    "        dZ1 = self.w2.T.dot(dZ2) * self.d_reLU(Z1)\n",
    "        dW1 = 1/m * dZ1.dot(X.T)\n",
    "        dB1 = 1/m * np.sum(dZ1,keepdims= True)\n",
    "        return dW1,dW2,dB1,dB2\n",
    "    def update_weights(self,lr,dW1,dW2,dB1,dB2):\n",
    "        self.w1 -= lr * dW1\n",
    "        self.b1 -= lr * dB1\n",
    "        self.w2 -= lr * dW2\n",
    "        self.b2 -= lr * dB2\n",
    "        \n",
    "\n",
    "    def predict(self, A2):\n",
    "        return np.argmax(A2,0)\n",
    "    def train(self,lr,X,Y):\n",
    "        Z1,A1,Z2,A2 = self.forward_propogation(X)\n",
    "        y_pred = self.predict(A2)\n",
    "        loss = self.mse_loss(Y,y_pred)\n",
    "        dW1, dW2, dB1, dB2 = self.backward_propogation(A1,Z1,A2,Z2,X,Y)\n",
    "        self.update_weights(lr,dW1,dW2,dB1,dB2)\n",
    "        return loss, y_pred\n",
    "    \n",
    "\n",
    "def accuracy(predictions, Y):\n",
    "    return np.sum(predictions == Y) / Y.size\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1558,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1561,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_normalize(x):\n",
    "    minx = X.min(axis=0)\n",
    "    xmax = X.max(axis=0)\n",
    "    Xnorm = (x - minx) / (xmax - minx)\n",
    "    return Xnorm\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to batch my data in order to increase efficency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1559,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7181818181818181\n",
      "Loss: 1.8227272727272728\n",
      "Accuracy: 0.2545454545454545\n",
      "Loss: 18.96818181818182\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.3909090909090909\n",
      "Loss: 5.513636363636364\n",
      "Accuracy: 0.3909090909090909\n",
      "Loss: 5.513636363636364\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.36818181818181817\n",
      "Loss: 6.172727272727273\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3909090909090909\n",
      "Loss: 4.422727272727273\n",
      "Accuracy: 0.4227272727272727\n",
      "Loss: 7.390909090909091\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.4818181818181818\n",
      "Loss: 7.490909090909091\n",
      "Accuracy: 0.3409090909090909\n",
      "Loss: 5.5636363636363635\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.3909090909090909\n",
      "Loss: 5.513636363636364\n",
      "Accuracy: 0.3909090909090909\n",
      "Loss: 5.513636363636364\n",
      "Accuracy: 0.3909090909090909\n",
      "Loss: 5.513636363636364\n",
      "Accuracy: 0.36363636363636365\n",
      "Loss: 5.540909090909091\n",
      "Accuracy: 0.35909090909090907\n",
      "Loss: 5.545454545454546\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.3409090909090909\n",
      "Loss: 5.5636363636363635\n",
      "Accuracy: 0.3090909090909091\n",
      "Loss: 5.595454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3136363636363636\n",
      "Loss: 5.590909090909091\n",
      "Accuracy: 0.3272727272727272\n",
      "Loss: 5.577272727272727\n",
      "Accuracy: 0.3045454545454545\n",
      "Loss: 5.6000000000000005\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.33636363636363636\n",
      "Loss: 5.568181818181818\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.3318181818181818\n",
      "Loss: 5.572727272727272\n",
      "Accuracy: 0.3181818181818182\n",
      "Loss: 5.586363636363637\n",
      "Accuracy: 0.3409090909090909\n",
      "Loss: 5.5636363636363635\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.3181818181818182\n",
      "Loss: 5.586363636363637\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3136363636363636\n",
      "Loss: 5.590909090909091\n",
      "Accuracy: 0.3318181818181818\n",
      "Loss: 5.572727272727272\n",
      "Accuracy: 0.3318181818181818\n",
      "Loss: 5.572727272727272\n",
      "Accuracy: 0.3318181818181818\n",
      "Loss: 5.572727272727272\n",
      "Accuracy: 0.3318181818181818\n",
      "Loss: 5.572727272727272\n",
      "Accuracy: 0.3181818181818182\n",
      "Loss: 5.586363636363637\n",
      "Accuracy: 0.3136363636363636\n",
      "Loss: 5.590909090909091\n",
      "Accuracy: 0.3090909090909091\n",
      "Loss: 5.595454545454545\n",
      "Accuracy: 0.3090909090909091\n",
      "Loss: 5.595454545454545\n",
      "Accuracy: 0.3181818181818182\n",
      "Loss: 5.586363636363637\n",
      "Accuracy: 0.3272727272727272\n",
      "Loss: 5.577272727272727\n",
      "Accuracy: 0.3272727272727272\n",
      "Loss: 5.577272727272727\n",
      "Accuracy: 0.3272727272727272\n",
      "Loss: 5.577272727272727\n",
      "Accuracy: 0.3272727272727272\n",
      "Loss: 5.577272727272727\n",
      "Accuracy: 0.3227272727272727\n",
      "Loss: 5.581818181818182\n",
      "Accuracy: 0.3272727272727272\n",
      "Loss: 5.577272727272727\n",
      "Accuracy: 0.3272727272727272\n",
      "Loss: 5.577272727272727\n",
      "Accuracy: 0.3272727272727272\n",
      "Loss: 5.577272727272727\n",
      "Accuracy: 0.3318181818181818\n",
      "Loss: 5.572727272727272\n",
      "Accuracy: 0.3318181818181818\n",
      "Loss: 5.572727272727272\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.3227272727272727\n",
      "Loss: 5.581818181818182\n",
      "Accuracy: 0.3045454545454545\n",
      "Loss: 5.6000000000000005\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3181818181818182\n",
      "Loss: 5.586363636363637\n",
      "Accuracy: 0.3318181818181818\n",
      "Loss: 5.572727272727272\n",
      "Accuracy: 0.3409090909090909\n",
      "Loss: 5.5636363636363635\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.36363636363636365\n",
      "Loss: 5.540909090909091\n",
      "Accuracy: 0.3909090909090909\n",
      "Loss: 5.513636363636364\n",
      "Accuracy: 0.3909090909090909\n",
      "Loss: 5.513636363636364\n",
      "Accuracy: 0.3909090909090909\n",
      "Loss: 5.513636363636364\n",
      "Accuracy: 0.4272727272727273\n",
      "Loss: 5.4772727272727275\n",
      "Accuracy: 0.4090909090909091\n",
      "Loss: 5.8136363636363635\n",
      "Accuracy: 0.3909090909090909\n",
      "Loss: 8.695454545454545\n",
      "Accuracy: 0.3909090909090909\n",
      "Loss: 8.695454545454545\n",
      "Accuracy: 0.3909090909090909\n",
      "Loss: 8.695454545454545\n",
      "Accuracy: 0.3909090909090909\n",
      "Loss: 8.695454545454545\n",
      "Accuracy: 0.41818181818181815\n",
      "Loss: 8.668181818181818\n",
      "Accuracy: 0.4272727272727273\n",
      "Loss: 8.659090909090908\n",
      "Accuracy: 0.43636363636363634\n",
      "Loss: 5.468181818181818\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3909090909090909\n",
      "Loss: 5.513636363636364\n",
      "Accuracy: 0.43636363636363634\n",
      "Loss: 5.468181818181818\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 5.472727272727273\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 5.472727272727273\n",
      "Accuracy: 0.43636363636363634\n",
      "Loss: 5.468181818181818\n",
      "Accuracy: 0.43636363636363634\n",
      "Loss: 5.468181818181818\n",
      "Accuracy: 0.43636363636363634\n",
      "Loss: 5.468181818181818\n",
      "Accuracy: 0.43636363636363634\n",
      "Loss: 5.468181818181818\n",
      "Accuracy: 0.4227272727272727\n",
      "Loss: 5.4818181818181815\n",
      "Accuracy: 0.3909090909090909\n",
      "Loss: 5.513636363636364\n",
      "Accuracy: 0.3909090909090909\n",
      "Loss: 5.513636363636364\n",
      "Accuracy: 0.39999999999999997\n",
      "Loss: 5.504545454545454\n",
      "Accuracy: 0.3909090909090909\n",
      "Loss: 5.513636363636364\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.4272727272727273\n",
      "Loss: 8.659090909090908\n",
      "Accuracy: 0.38181818181818183\n",
      "Loss: 5.5227272727272725\n",
      "Accuracy: 0.43636363636363634\n",
      "Loss: 5.468181818181818\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.3727272727272727\n",
      "Loss: 5.531818181818182\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.3409090909090909\n",
      "Loss: 5.5636363636363635\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.3909090909090909\n",
      "Loss: 5.513636363636364\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.33636363636363636\n",
      "Loss: 5.568181818181818\n",
      "Accuracy: 0.33636363636363636\n",
      "Loss: 5.568181818181818\n",
      "Accuracy: 0.3909090909090909\n",
      "Loss: 5.513636363636364\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.3272727272727272\n",
      "Loss: 5.577272727272727\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.43636363636363634\n",
      "Loss: 7.377272727272728\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3227272727272727\n",
      "Loss: 5.581818181818182\n",
      "Accuracy: 0.43636363636363634\n",
      "Loss: 7.0590909090909095\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.3136363636363636\n",
      "Loss: 5.590909090909091\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.3909090909090909\n",
      "Loss: 5.513636363636364\n",
      "Accuracy: 0.3909090909090909\n",
      "Loss: 5.513636363636364\n",
      "Accuracy: 0.3909090909090909\n",
      "Loss: 5.513636363636364\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.3909090909090909\n",
      "Loss: 4.8999999999999995\n",
      "Accuracy: 0.43636363636363634\n",
      "Loss: 5.968181818181819\n",
      "Accuracy: 0.46818181818181814\n",
      "Loss: 6.763636363636365\n",
      "Accuracy: 0.4818181818181818\n",
      "Loss: 7.513636363636365\n",
      "Accuracy: 0.43636363636363634\n",
      "Loss: 9.15\n",
      "Accuracy: 0.3772727272727273\n",
      "Loss: 10.299999999999999\n",
      "Accuracy: 0.41818181818181815\n",
      "Loss: 5.804545454545455\n",
      "Accuracy: 0.43636363636363634\n",
      "Loss: 5.468181818181818\n",
      "Accuracy: 0.3909090909090909\n",
      "Loss: 5.513636363636364\n",
      "Accuracy: 0.39545454545454545\n",
      "Loss: 5.509090909090909\n",
      "Accuracy: 0.39545454545454545\n",
      "Loss: 5.509090909090909\n",
      "Accuracy: 0.40454545454545454\n",
      "Loss: 5.5\n",
      "Accuracy: 0.39545454545454545\n",
      "Loss: 5.509090909090909\n",
      "Accuracy: 0.3909090909090909\n",
      "Loss: 5.513636363636364\n",
      "Accuracy: 0.3909090909090909\n",
      "Loss: 5.513636363636364\n",
      "Accuracy: 0.43636363636363634\n",
      "Loss: 5.468181818181818\n",
      "Accuracy: 0.43636363636363634\n",
      "Loss: 8.65\n",
      "Accuracy: 0.3909090909090909\n",
      "Loss: 5.513636363636364\n",
      "Accuracy: 0.35909090909090907\n",
      "Loss: 5.545454545454546\n",
      "Accuracy: 0.4272727272727273\n",
      "Loss: 5.4772727272727275\n",
      "Accuracy: 0.3909090909090909\n",
      "Loss: 5.513636363636364\n",
      "Accuracy: 0.4227272727272727\n",
      "Loss: 5.4818181818181815\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.4227272727272727\n",
      "Loss: 7.55\n",
      "Accuracy: 0.3909090909090909\n",
      "Loss: 4.422727272727273\n",
      "Accuracy: 0.440909090909091\n",
      "Loss: 7.054545454545454\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.43636363636363634\n",
      "Loss: 4.377272727272727\n",
      "Accuracy: 0.4818181818181818\n",
      "Loss: 7.013636363636365\n",
      "Accuracy: 0.43636363636363634\n",
      "Loss: 4.377272727272727\n",
      "Accuracy: 0.43636363636363634\n",
      "Loss: 6.263636363636365\n",
      "Accuracy: 0.45909090909090905\n",
      "Loss: 7.036363636363637\n",
      "Accuracy: 0.5727272727272728\n",
      "Loss: 4.295454545454546\n",
      "Accuracy: 0.43636363636363634\n",
      "Loss: 8.65\n",
      "Accuracy: 0.4818181818181818\n",
      "Loss: 3.2409090909090907\n",
      "Accuracy: 0.5863636363636364\n",
      "Loss: 4.7272727272727275\n",
      "Accuracy: 0.5863636363636364\n",
      "Loss: 5.577272727272727\n",
      "Accuracy: 0.4818181818181818\n",
      "Loss: 7.513636363636365\n",
      "Accuracy: 0.5863636363636364\n",
      "Loss: 5.577272727272727\n",
      "Accuracy: 0.4818181818181818\n",
      "Loss: 8.604545454545455\n",
      "Accuracy: 0.43636363636363634\n",
      "Loss: 5.468181818181818\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.36363636363636365\n",
      "Loss: 5.540909090909091\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.3909090909090909\n",
      "Loss: 5.513636363636364\n",
      "Accuracy: 0.35454545454545455\n",
      "Loss: 5.55\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.33636363636363636\n",
      "Loss: 5.568181818181818\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.3181818181818182\n",
      "Loss: 5.586363636363637\n",
      "Accuracy: 0.3090909090909091\n",
      "Loss: 5.595454545454545\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.3909090909090909\n",
      "Loss: 5.513636363636364\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.43636363636363634\n",
      "Loss: 5.468181818181818\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3136363636363636\n",
      "Loss: 5.590909090909091\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3136363636363636\n",
      "Loss: 5.590909090909091\n",
      "Accuracy: 0.3318181818181818\n",
      "Loss: 5.572727272727272\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.3181818181818182\n",
      "Loss: 5.586363636363637\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3045454545454545\n",
      "Loss: 5.6000000000000005\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.3909090909090909\n",
      "Loss: 5.513636363636364\n",
      "Accuracy: 0.3909090909090909\n",
      "Loss: 5.513636363636364\n",
      "Accuracy: 0.3909090909090909\n",
      "Loss: 5.513636363636364\n",
      "Accuracy: 0.3909090909090909\n",
      "Loss: 5.513636363636364\n",
      "Accuracy: 0.3909090909090909\n",
      "Loss: 6.627272727272728\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 6.990909090909091\n",
      "Accuracy: 0.43636363636363634\n",
      "Loss: 5.945454545454546\n",
      "Accuracy: 0.3227272727272727\n",
      "Loss: 5.581818181818182\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.38181818181818183\n",
      "Loss: 5.5227272727272725\n",
      "Accuracy: 0.35454545454545455\n",
      "Loss: 5.55\n",
      "Accuracy: 0.43636363636363634\n",
      "Loss: 5.468181818181818\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.3909090909090909\n",
      "Loss: 5.513636363636364\n",
      "Accuracy: 0.3136363636363636\n",
      "Loss: 5.590909090909091\n",
      "Accuracy: 0.3909090909090909\n",
      "Loss: 5.513636363636364\n",
      "Accuracy: 0.43636363636363634\n",
      "Loss: 5.468181818181818\n",
      "Accuracy: 0.35454545454545455\n",
      "Loss: 5.55\n",
      "Accuracy: 0.3909090909090909\n",
      "Loss: 5.513636363636364\n",
      "Accuracy: 0.43636363636363634\n",
      "Loss: 5.468181818181818\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3909090909090909\n",
      "Loss: 5.513636363636364\n",
      "Accuracy: 0.43636363636363634\n",
      "Loss: 5.468181818181818\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3181818181818182\n",
      "Loss: 5.586363636363637\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.36818181818181817\n",
      "Loss: 5.536363636363636\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.3909090909090909\n",
      "Loss: 5.513636363636364\n",
      "Accuracy: 0.43636363636363634\n",
      "Loss: 5.468181818181818\n",
      "Accuracy: 0.3909090909090909\n",
      "Loss: 5.513636363636364\n",
      "Accuracy: 0.43636363636363634\n",
      "Loss: 5.468181818181818\n",
      "Accuracy: 0.40454545454545454\n",
      "Loss: 5.5\n",
      "Accuracy: 0.3909090909090909\n",
      "Loss: 5.513636363636364\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.29545454545454547\n",
      "Loss: 5.609090909090909\n",
      "Accuracy: 0.2818181818181818\n",
      "Loss: 5.622727272727273\n",
      "Accuracy: 0.2636363636363636\n",
      "Loss: 5.640909090909091\n",
      "Accuracy: 0.2727272727272727\n",
      "Loss: 5.631818181818182\n",
      "Accuracy: 0.2909090909090909\n",
      "Loss: 5.613636363636363\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3\n",
      "Loss: 5.6045454545454545\n",
      "Accuracy: 0.3181818181818182\n",
      "Loss: 5.586363636363637\n",
      "Accuracy: 0.3318181818181818\n",
      "Loss: 5.572727272727272\n",
      "Accuracy: 0.33636363636363636\n",
      "Loss: 5.568181818181818\n",
      "Accuracy: 0.3409090909090909\n",
      "Loss: 5.5636363636363635\n",
      "Accuracy: 0.3409090909090909\n",
      "Loss: 5.5636363636363635\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.34545454545454546\n",
      "Loss: 5.559090909090909\n",
      "Accuracy: 0.3568181818181818\n",
      "Loss: 5.2749999999999995\n",
      "Accuracy: 0.3568181818181818\n",
      "Loss: 5.2749999999999995\n",
      "Accuracy: 0.3568181818181818\n",
      "Loss: 5.2749999999999995\n",
      "Accuracy: 0.3568181818181818\n",
      "Loss: 5.2749999999999995\n",
      "Accuracy: 0.3568181818181818\n",
      "Loss: 5.2749999999999995\n",
      "Accuracy: 0.3568181818181818\n",
      "Loss: 5.2749999999999995\n",
      "Accuracy: 0.3568181818181818\n",
      "Loss: 5.2749999999999995\n",
      "Accuracy: 0.3568181818181818\n",
      "Loss: 5.2749999999999995\n",
      "Accuracy: 0.3568181818181818\n",
      "Loss: 5.2749999999999995\n",
      "Accuracy: 0.3568181818181818\n",
      "Loss: 5.2749999999999995\n",
      "Accuracy: 0.3568181818181818\n",
      "Loss: 5.2749999999999995\n",
      "Accuracy: 0.3568181818181818\n",
      "Loss: 5.2749999999999995\n",
      "Accuracy: 0.3568181818181818\n",
      "Loss: 5.2749999999999995\n",
      "Accuracy: 0.3568181818181818\n",
      "Loss: 5.2749999999999995\n",
      "Accuracy: 0.3568181818181818\n",
      "Loss: 5.2749999999999995\n",
      "Accuracy: 0.3568181818181818\n",
      "Loss: 5.2749999999999995\n",
      "Accuracy: 0.3568181818181818\n",
      "Loss: 5.2749999999999995\n",
      "Accuracy: 0.3568181818181818\n",
      "Loss: 5.2749999999999995\n",
      "Accuracy: 0.36136363636363633\n",
      "Loss: 5.161363636363636\n",
      "Accuracy: 0.36136363636363633\n",
      "Loss: 5.161363636363636\n",
      "Accuracy: 0.36590909090909096\n",
      "Loss: 5.047727272727273\n",
      "Accuracy: 0.36590909090909096\n",
      "Loss: 5.047727272727273\n",
      "Accuracy: 0.36590909090909096\n",
      "Loss: 5.047727272727273\n",
      "Accuracy: 0.36590909090909096\n",
      "Loss: 5.047727272727273\n",
      "Accuracy: 0.375\n",
      "Loss: 4.820454545454545\n",
      "Accuracy: 0.375\n",
      "Loss: 4.820454545454545\n",
      "Accuracy: 0.375\n",
      "Loss: 4.820454545454545\n",
      "Accuracy: 0.38409090909090904\n",
      "Loss: 4.593181818181818\n",
      "Accuracy: 0.3886363636363636\n",
      "Loss: 4.4795454545454545\n",
      "Accuracy: 0.3886363636363636\n",
      "Loss: 4.4795454545454545\n",
      "Accuracy: 0.3886363636363636\n",
      "Loss: 4.4795454545454545\n",
      "Accuracy: 0.3886363636363636\n",
      "Loss: 4.4795454545454545\n",
      "Accuracy: 0.3886363636363636\n",
      "Loss: 4.4795454545454545\n",
      "Accuracy: 0.3886363636363636\n",
      "Loss: 4.4795454545454545\n",
      "Accuracy: 0.3931818181818182\n",
      "Loss: 4.365909090909091\n",
      "Accuracy: 0.3977272727272727\n",
      "Loss: 4.252272727272727\n",
      "Accuracy: 0.3977272727272727\n",
      "Loss: 4.252272727272727\n",
      "Accuracy: 0.3977272727272727\n",
      "Loss: 4.252272727272727\n",
      "Accuracy: 0.3977272727272727\n",
      "Loss: 4.252272727272727\n",
      "Accuracy: 0.3977272727272727\n",
      "Loss: 4.252272727272727\n",
      "Accuracy: 0.3977272727272727\n",
      "Loss: 4.252272727272727\n",
      "Accuracy: 0.3977272727272727\n",
      "Loss: 4.252272727272727\n",
      "Accuracy: 0.3977272727272727\n",
      "Loss: 4.252272727272727\n",
      "Accuracy: 0.3977272727272727\n",
      "Loss: 4.252272727272727\n",
      "Accuracy: 0.3977272727272727\n",
      "Loss: 4.252272727272727\n",
      "Accuracy: 0.3977272727272727\n",
      "Loss: 4.252272727272727\n",
      "Accuracy: 0.3977272727272727\n",
      "Loss: 4.252272727272727\n",
      "Accuracy: 0.3977272727272727\n",
      "Loss: 4.252272727272727\n",
      "Accuracy: 0.3977272727272727\n",
      "Loss: 4.252272727272727\n",
      "Accuracy: 0.3977272727272727\n",
      "Loss: 4.252272727272727\n",
      "Accuracy: 0.3977272727272727\n",
      "Loss: 4.252272727272727\n",
      "Accuracy: 0.3977272727272727\n",
      "Loss: 4.252272727272727\n",
      "Accuracy: 0.42045454545454547\n",
      "Loss: 3.684090909090909\n",
      "Accuracy: 0.42045454545454547\n",
      "Loss: 3.684090909090909\n",
      "Accuracy: 0.42045454545454547\n",
      "Loss: 3.684090909090909\n",
      "Accuracy: 0.42045454545454547\n",
      "Loss: 3.684090909090909\n",
      "Accuracy: 0.42045454545454547\n",
      "Loss: 3.684090909090909\n",
      "Accuracy: 0.3909090909090909\n",
      "Loss: 4.422727272727273\n",
      "Accuracy: 0.44090909090909086\n",
      "Loss: 3.3909090909090907\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4227272727272727\n",
      "Loss: 3.627272727272727\n",
      "Accuracy: 0.5045454545454545\n",
      "Loss: 2.6727272727272724\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.4318181818181818\n",
      "Loss: 3.4\n",
      "Accuracy: 0.43636363636363634\n",
      "Loss: 3.286363636363636\n",
      "Accuracy: 0.43636363636363634\n",
      "Loss: 3.286363636363636\n",
      "Accuracy: 0.43636363636363634\n",
      "Loss: 3.286363636363636\n",
      "Accuracy: 0.43636363636363634\n",
      "Loss: 3.286363636363636\n",
      "Accuracy: 0.44090909090909086\n",
      "Loss: 3.1727272727272724\n",
      "Accuracy: 0.44090909090909086\n",
      "Loss: 3.1727272727272724\n",
      "Accuracy: 0.4454545454545454\n",
      "Loss: 3.059090909090909\n",
      "Accuracy: 0.4454545454545454\n",
      "Loss: 3.059090909090909\n",
      "Accuracy: 0.45\n",
      "Loss: 2.9454545454545453\n",
      "Accuracy: 0.45454545454545453\n",
      "Loss: 2.8318181818181816\n",
      "Accuracy: 0.45454545454545453\n",
      "Loss: 2.8318181818181816\n",
      "Accuracy: 0.45454545454545453\n",
      "Loss: 2.8318181818181816\n",
      "Accuracy: 0.45909090909090905\n",
      "Loss: 2.7181818181818183\n",
      "Accuracy: 0.45909090909090905\n",
      "Loss: 2.7181818181818183\n",
      "Accuracy: 0.45909090909090905\n",
      "Loss: 2.7181818181818183\n",
      "Accuracy: 0.4681818181818182\n",
      "Loss: 2.4909090909090907\n",
      "Accuracy: 0.4636363636363636\n",
      "Loss: 2.6045454545454545\n",
      "Accuracy: 0.4681818181818182\n",
      "Loss: 2.4909090909090907\n",
      "Accuracy: 0.4681818181818182\n",
      "Loss: 2.4909090909090907\n",
      "Accuracy: 0.4681818181818182\n",
      "Loss: 2.4909090909090907\n",
      "Accuracy: 0.4681818181818182\n",
      "Loss: 2.4909090909090907\n",
      "Accuracy: 0.4681818181818182\n",
      "Loss: 2.4909090909090907\n",
      "Accuracy: 0.4681818181818182\n",
      "Loss: 2.4909090909090907\n",
      "Accuracy: 0.4681818181818182\n",
      "Loss: 2.4909090909090907\n",
      "Accuracy: 0.4681818181818182\n",
      "Loss: 2.4909090909090907\n",
      "Accuracy: 0.4681818181818182\n",
      "Loss: 2.4909090909090907\n",
      "Accuracy: 0.4681818181818182\n",
      "Loss: 2.4909090909090907\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4727272727272727\n",
      "Loss: 2.377272727272727\n",
      "Accuracy: 0.4772727272727273\n",
      "Loss: 2.2636363636363637\n",
      "Accuracy: 0.4772727272727273\n",
      "Loss: 2.2636363636363637\n"
     ]
    }
   ],
   "source": [
    "for j in range(1000):\n",
    "    total_acc = 0\n",
    "    total_loss = 0\n",
    "    for i in range(len(X_train)):\n",
    "        loss, y_pred = nn.train(.1,min_max_normalize(X_train[i].T),Y_train[i].T)\n",
    "        acc = (accuracy(y_pred, Y_train[i].T))\n",
    "        total_acc += acc\n",
    "        total_loss += loss\n",
    "    print(f'Accuracy: {total_acc/len(X_train)}')\n",
    "    print(f'Loss: {total_loss/len(X_train)}')\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2: "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1: I needed the nn import in order to create my neural network and Data Loader in order to get my csv into a good formate for training. Many of the examples on PyTorch use this. The optim import is just for my optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1067,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn as n\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.optim as optim\n",
    "import copy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2 from here below: Train and Test Datasets needed some work from just being numpy arrays. When I tried originally with them, I was running into issues with types. Doing it by converting to torch tensors with torch dtypes fixed a lot of my issues. I then changed them into to tensor datasets in order to have my values together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1582,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('glass.csv')\n",
    "X = df.drop('Type', axis=1).values\n",
    "Y = df['Type'].values\n",
    "\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "Y_tensor = torch.tensor(Y, dtype=torch.long)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_tensor, Y_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = TensorDataset(X_train, Y_train)\n",
    "test_dataset = TensorDataset(X_test, Y_test)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1589,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(n.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.seq = n.Sequential(n.Linear(9, 128)\n",
    "        ,n.ReLU(),\n",
    "        n.Linear(128, 128),\n",
    "        n.ReLU(),\n",
    "        n.Linear(128, 8))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #The forward propogation. Im using relu as my activation function\n",
    "        return self.seq(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1590,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1591,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "optimizer = optim.Adam(model.parameters(),lr)\n",
    "loss = n.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1592,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loss_fn, optimizer, train_loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, Y_batch in train_loader:\n",
    "        # this was my gradient descent We would calculate the loss and thenn use it to do the backward propogation. We would calculate the total lost by the end\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch)\n",
    "        loss = loss_fn(output, Y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f'Training loss: {avg_loss}')\n",
    "\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, Y_batch in test_loader:\n",
    "            output = model(X_batch)\n",
    "            #This is equivalent to a softmax function to find the actual prediction\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += Y_batch.size(0)\n",
    "            correct += (predicted == Y_batch).sum().item()\n",
    "    print(f'Accuracy: {100 * correct / total}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1594,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Training loss: 1.4749383926391602\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 2\n",
      "Training loss: 1.470792060548609\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 3\n",
      "Training loss: 1.473633581941778\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 4\n",
      "Training loss: 1.4982598532329907\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 5\n",
      "Training loss: 1.4798301187428562\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 6\n",
      "Training loss: 1.501905769109726\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 7\n",
      "Training loss: 1.4774472117424011\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 8\n",
      "Training loss: 1.4710200862451033\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 9\n",
      "Training loss: 1.468632698059082\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 10\n",
      "Training loss: 1.477489021691409\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 11\n",
      "Training loss: 1.4955509034070102\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 12\n",
      "Training loss: 1.4771831252358176\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 13\n",
      "Training loss: 1.5017394856973127\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 14\n",
      "Training loss: 1.486597004261884\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 15\n",
      "Training loss: 1.497665600343184\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 16\n",
      "Training loss: 1.5143876671791077\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 17\n",
      "Training loss: 1.5113538015972485\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 18\n",
      "Training loss: 1.4997142932631753\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 19\n",
      "Training loss: 1.4930789199742405\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 20\n",
      "Training loss: 1.4698009816083042\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 21\n",
      "Training loss: 1.4701639847321943\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 22\n",
      "Training loss: 1.4724159078164534\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 23\n",
      "Training loss: 1.4866317185488613\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 24\n",
      "Training loss: 1.4947859970006077\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 25\n",
      "Training loss: 1.477748383175243\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 26\n",
      "Training loss: 1.482198628512296\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 27\n",
      "Training loss: 1.4709294546734204\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 28\n",
      "Training loss: 1.4956263574686917\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 29\n",
      "Training loss: 1.4775153940374202\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 30\n",
      "Training loss: 1.4820197332989087\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 31\n",
      "Training loss: 1.501567927273837\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 32\n",
      "Training loss: 1.4861023751172153\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 33\n",
      "Training loss: 1.5057957172393799\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 34\n",
      "Training loss: 1.4898883971300991\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 35\n",
      "Training loss: 1.4858224554495378\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 36\n",
      "Training loss: 1.4787191640246997\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 37\n",
      "Training loss: 1.4943335381421177\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 38\n",
      "Training loss: 1.4978864138776606\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 39\n",
      "Training loss: 1.4744854677807202\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 40\n",
      "Training loss: 1.480290949344635\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 41\n",
      "Training loss: 1.4927322133020922\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 42\n",
      "Training loss: 1.5110683603720232\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 43\n",
      "Training loss: 1.4987765550613403\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 44\n",
      "Training loss: 1.5425770716233687\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 45\n",
      "Training loss: 1.4925289045680652\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 46\n",
      "Training loss: 1.4888531077991833\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 47\n",
      "Training loss: 1.482710681178353\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 48\n",
      "Training loss: 1.4783875346183777\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 49\n",
      "Training loss: 1.484680560502139\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 50\n",
      "Training loss: 1.4931215860626914\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 51\n",
      "Training loss: 1.501883241263303\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 52\n",
      "Training loss: 1.492449933832342\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 53\n",
      "Training loss: 1.4767108667980542\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 54\n",
      "Training loss: 1.4951023567806592\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 55\n",
      "Training loss: 1.4731678475033154\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 56\n",
      "Training loss: 1.4779982566833496\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 57\n",
      "Training loss: 1.5173641389066523\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 58\n",
      "Training loss: 1.4966964775865728\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 59\n",
      "Training loss: 1.4727674722671509\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 60\n",
      "Training loss: 1.482232077555223\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 61\n",
      "Training loss: 1.4777720353820107\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 62\n",
      "Training loss: 1.4955650215799159\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 63\n",
      "Training loss: 1.4706667444922707\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 64\n",
      "Training loss: 1.4932623451406306\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 65\n",
      "Training loss: 1.5014042204076594\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 66\n",
      "Training loss: 1.4694779840382663\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 67\n",
      "Training loss: 1.4855865781957454\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 68\n",
      "Training loss: 1.4937464052980596\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 69\n",
      "Training loss: 1.468808027830991\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 70\n",
      "Training loss: 1.5003775629130276\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 71\n",
      "Training loss: 1.5138469446789136\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 72\n",
      "Training loss: 1.491161042993719\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 73\n",
      "Training loss: 1.4982471899552778\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 74\n",
      "Training loss: 1.4800908294591038\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 75\n",
      "Training loss: 1.492329109798778\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 76\n",
      "Training loss: 1.5001725446094165\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 77\n",
      "Training loss: 1.4914906133304944\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 78\n",
      "Training loss: 1.5297497673468157\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 79\n",
      "Training loss: 1.4897581663998691\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 80\n",
      "Training loss: 1.472272965041074\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 81\n",
      "Training loss: 1.4719074151732705\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 82\n",
      "Training loss: 1.5080805773084813\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 83\n",
      "Training loss: 1.5020285953174939\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 84\n",
      "Training loss: 1.4937367981130427\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 85\n",
      "Training loss: 1.4698089523748918\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 86\n",
      "Training loss: 1.5008001869375056\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 87\n",
      "Training loss: 1.4906090606342663\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 88\n",
      "Training loss: 1.4814128442244097\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 89\n",
      "Training loss: 1.488597027280114\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 90\n",
      "Training loss: 1.513254387812181\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 91\n",
      "Training loss: 1.4840618913823909\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 92\n",
      "Training loss: 1.4772324507886714\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 93\n",
      "Training loss: 1.4860443635420366\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 94\n",
      "Training loss: 1.4878009286793796\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 95\n",
      "Training loss: 1.4885614351792769\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 96\n",
      "Training loss: 1.4855958006598733\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 97\n",
      "Training loss: 1.509632023898038\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 98\n",
      "Training loss: 1.4769990877671675\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 99\n",
      "Training loss: 1.4966719204729253\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 100\n",
      "Training loss: 1.5045300342819907\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 101\n",
      "Training loss: 1.4768537012013523\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 102\n",
      "Training loss: 1.4942697936838323\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 103\n",
      "Training loss: 1.5071289322592996\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 104\n",
      "Training loss: 1.4751235138286243\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 105\n",
      "Training loss: 1.4919353127479553\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 106\n",
      "Training loss: 1.4856418479572644\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 107\n",
      "Training loss: 1.4647891142151572\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 108\n",
      "Training loss: 1.4860619740052656\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 109\n",
      "Training loss: 1.4754183725877241\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 110\n",
      "Training loss: 1.4911026629534634\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 111\n",
      "Training loss: 1.4735766920176419\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 112\n",
      "Training loss: 1.486567410555753\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 113\n",
      "Training loss: 1.4849463376131924\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 114\n",
      "Training loss: 1.5071743293242021\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 115\n",
      "Training loss: 1.5098296187140725\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 116\n",
      "Training loss: 1.4924184571612964\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 117\n",
      "Training loss: 1.4682836532592773\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 118\n",
      "Training loss: 1.496191306547685\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 119\n",
      "Training loss: 1.5095249793746255\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 120\n",
      "Training loss: 1.4778957475315442\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 121\n",
      "Training loss: 1.4787227728150107\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 122\n",
      "Training loss: 1.467623607678847\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 123\n",
      "Training loss: 1.470984082330357\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 124\n",
      "Training loss: 1.4701156182722612\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 125\n",
      "Training loss: 1.5091299197890542\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 126\n",
      "Training loss: 1.4763193130493164\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 127\n",
      "Training loss: 1.4729767116633328\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 128\n",
      "Training loss: 1.5031837062402205\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 129\n",
      "Training loss: 1.4853350574320012\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 130\n",
      "Training loss: 1.4685825922272422\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 131\n",
      "Training loss: 1.4787437915802002\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 132\n",
      "Training loss: 1.4825758067044346\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 133\n",
      "Training loss: 1.4709368727423928\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 134\n",
      "Training loss: 1.4918781248005955\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 135\n",
      "Training loss: 1.4867725859988818\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 136\n",
      "Training loss: 1.462453219023618\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 137\n",
      "Training loss: 1.5005146102471785\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 138\n",
      "Training loss: 1.4787958752025256\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 139\n",
      "Training loss: 1.502952521497553\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 140\n",
      "Training loss: 1.4921295859596946\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 141\n",
      "Training loss: 1.50031179189682\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 142\n",
      "Training loss: 1.4963827891783281\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 143\n",
      "Training loss: 1.48084865916859\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 144\n",
      "Training loss: 1.4974784038283608\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 145\n",
      "Training loss: 1.490787766196511\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 146\n",
      "Training loss: 1.472386211156845\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 147\n",
      "Training loss: 1.505943851037459\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 148\n",
      "Training loss: 1.4765111587264321\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 149\n",
      "Training loss: 1.487494246526198\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 150\n",
      "Training loss: 1.4751669385216453\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 151\n",
      "Training loss: 1.514673189683394\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 152\n",
      "Training loss: 1.490025352347981\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 153\n",
      "Training loss: 1.4938099438493901\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 154\n",
      "Training loss: 1.4761057116768577\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 155\n",
      "Training loss: 1.4860882650722156\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 156\n",
      "Training loss: 1.4969589439305393\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 157\n",
      "Training loss: 1.4832927638834172\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 158\n",
      "Training loss: 1.4971274462613193\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 159\n",
      "Training loss: 1.4792812195691196\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 160\n",
      "Training loss: 1.5144250176169656\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 161\n",
      "Training loss: 1.4828905354846607\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 162\n",
      "Training loss: 1.4688587676395068\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 163\n",
      "Training loss: 1.5000077865340493\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 164\n",
      "Training loss: 1.485161380334334\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 165\n",
      "Training loss: 1.4851198900829663\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 166\n",
      "Training loss: 1.4719028256156228\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 167\n",
      "Training loss: 1.4930878769267688\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 168\n",
      "Training loss: 1.4868902238932522\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 169\n",
      "Training loss: 1.5081268007105046\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 170\n",
      "Training loss: 1.4740408604795283\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 171\n",
      "Training loss: 1.4951495311476968\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 172\n",
      "Training loss: 1.4942763962528922\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 173\n",
      "Training loss: 1.4787309657443652\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 174\n",
      "Training loss: 1.4828985170884565\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 175\n",
      "Training loss: 1.4761790524829517\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 176\n",
      "Training loss: 1.497168698094108\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 177\n",
      "Training loss: 1.4773824973539873\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 178\n",
      "Training loss: 1.4783922000364824\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 179\n",
      "Training loss: 1.5099935260686008\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 180\n",
      "Training loss: 1.4706914262338118\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 181\n",
      "Training loss: 1.5005831610072742\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 182\n",
      "Training loss: 1.5003199089657178\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 183\n",
      "Training loss: 1.486160159111023\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 184\n",
      "Training loss: 1.4815123595974662\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 185\n",
      "Training loss: 1.493345780806108\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 186\n",
      "Training loss: 1.4740661978721619\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 187\n",
      "Training loss: 1.4718658111312173\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 188\n",
      "Training loss: 1.5094172304326838\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 189\n",
      "Training loss: 1.475468012419614\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 190\n",
      "Training loss: 1.4925434697758069\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 191\n",
      "Training loss: 1.5016444000330837\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 192\n",
      "Training loss: 1.4974393302744085\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 193\n",
      "Training loss: 1.492007932879708\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 194\n",
      "Training loss: 1.5019745447418906\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 195\n",
      "Training loss: 1.4788324995474382\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 196\n",
      "Training loss: 1.4971648129549893\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 197\n",
      "Training loss: 1.480834261937575\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 198\n",
      "Training loss: 1.5094651742414995\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 199\n",
      "Training loss: 1.5014575936577537\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 200\n",
      "Training loss: 1.473785319111564\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 201\n",
      "Training loss: 1.5132687525315718\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 202\n",
      "Training loss: 1.4662097800861706\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 203\n",
      "Training loss: 1.4716945724053816\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 204\n",
      "Training loss: 1.5124884085221724\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 205\n",
      "Training loss: 1.4912934682585977\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 206\n",
      "Training loss: 1.4776273044672878\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 207\n",
      "Training loss: 1.4871104034510525\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 208\n",
      "Training loss: 1.4764982516115361\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 209\n",
      "Training loss: 1.4679618640379473\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 210\n",
      "Training loss: 1.4741658189079978\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 211\n",
      "Training loss: 1.48501487753608\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 212\n",
      "Training loss: 1.4689293232831089\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 213\n",
      "Training loss: 1.485576564615423\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 214\n",
      "Training loss: 1.5078602541576733\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 215\n",
      "Training loss: 1.4832707426764749\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 216\n",
      "Training loss: 1.4819603670727124\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 217\n",
      "Training loss: 1.468814806504683\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 218\n",
      "Training loss: 1.5005055449225686\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 219\n",
      "Training loss: 1.4693539955399253\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 220\n",
      "Training loss: 1.4944139231335034\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 221\n",
      "Training loss: 1.529069255698811\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 222\n",
      "Training loss: 1.4844454445622184\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 223\n",
      "Training loss: 1.51367496360432\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 224\n",
      "Training loss: 1.4772141142324968\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 225\n",
      "Training loss: 1.4888043024323203\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 226\n",
      "Training loss: 1.5087696801532398\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 227\n",
      "Training loss: 1.4982589970935474\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 228\n",
      "Training loss: 1.4791227254000576\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 229\n",
      "Training loss: 1.473597526550293\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 230\n",
      "Training loss: 1.511159685525027\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 231\n",
      "Training loss: 1.4880984750660984\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 232\n",
      "Training loss: 1.48293536901474\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 233\n",
      "Training loss: 1.4761234982447191\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 234\n",
      "Training loss: 1.4754124879837036\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 235\n",
      "Training loss: 1.4670468026941472\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 236\n",
      "Training loss: 1.5138277438553898\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 237\n",
      "Training loss: 1.472938965667378\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 238\n",
      "Training loss: 1.4978046742352573\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 239\n",
      "Training loss: 1.491403265432878\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 240\n",
      "Training loss: 1.4854283007708462\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 241\n",
      "Training loss: 1.478947016325864\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 242\n",
      "Training loss: 1.488929569721222\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 243\n",
      "Training loss: 1.5092114806175232\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 244\n",
      "Training loss: 1.484711305661635\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 245\n",
      "Training loss: 1.4932705055583606\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 246\n",
      "Training loss: 1.4914301362904636\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 247\n",
      "Training loss: 1.4968512708490544\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 248\n",
      "Training loss: 1.4854222752831199\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 249\n",
      "Training loss: 1.476180374622345\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 250\n",
      "Training loss: 1.482253535227342\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 251\n",
      "Training loss: 1.470922220836986\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 252\n",
      "Training loss: 1.4836918278173967\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 253\n",
      "Training loss: 1.4910392381928184\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 254\n",
      "Training loss: 1.5005190480839123\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 255\n",
      "Training loss: 1.4771116863597522\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 256\n",
      "Training loss: 1.492304731499065\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 257\n",
      "Training loss: 1.4741043827750466\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 258\n",
      "Training loss: 1.473710060119629\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 259\n",
      "Training loss: 1.4909843368963762\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 260\n",
      "Training loss: 1.4816874319856816\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 261\n",
      "Training loss: 1.4686613137071782\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 262\n",
      "Training loss: 1.4953173995018005\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 263\n",
      "Training loss: 1.4891768856482073\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 264\n",
      "Training loss: 1.49618830464103\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 265\n",
      "Training loss: 1.5089536905288696\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 266\n",
      "Training loss: 1.5150564854795283\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 267\n",
      "Training loss: 1.4933163252743809\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 268\n",
      "Training loss: 1.4677278724583713\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 269\n",
      "Training loss: 1.48732482845133\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 270\n",
      "Training loss: 1.4914065111767163\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 271\n",
      "Training loss: 1.4786076112227007\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 272\n",
      "Training loss: 1.5011527700857683\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 273\n",
      "Training loss: 1.4724296548149802\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 274\n",
      "Training loss: 1.5059423880143599\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 275\n",
      "Training loss: 1.5049212087284436\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 276\n",
      "Training loss: 1.491154432296753\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 277\n",
      "Training loss: 1.5144807425412266\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 278\n",
      "Training loss: 1.4920178109949285\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 279\n",
      "Training loss: 1.4726570682092146\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 280\n",
      "Training loss: 1.4968930374492297\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 281\n",
      "Training loss: 1.4854950633915989\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 282\n",
      "Training loss: 1.4885952039198442\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 283\n",
      "Training loss: 1.491227542812174\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 284\n",
      "Training loss: 1.4876061461188577\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 285\n",
      "Training loss: 1.4855824004520068\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 286\n",
      "Training loss: 1.4848839965733616\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 287\n",
      "Training loss: 1.4904059334234758\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 288\n",
      "Training loss: 1.4841255437244067\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 289\n",
      "Training loss: 1.4776098782365972\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 290\n",
      "Training loss: 1.4958188750527122\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 291\n",
      "Training loss: 1.4773640524257312\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 292\n",
      "Training loss: 1.5170494534752585\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 293\n",
      "Training loss: 1.4809453866698525\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 294\n",
      "Training loss: 1.4882794618606567\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 295\n",
      "Training loss: 1.4929315772923557\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 296\n",
      "Training loss: 1.5086304328658364\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 297\n",
      "Training loss: 1.5103529312393882\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 298\n",
      "Training loss: 1.4891246177933433\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 299\n",
      "Training loss: 1.4676012342626399\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 300\n",
      "Training loss: 1.4700726649977944\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 301\n",
      "Training loss: 1.4818258014592258\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 302\n",
      "Training loss: 1.500378673726862\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 303\n",
      "Training loss: 1.5225297808647156\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 304\n",
      "Training loss: 1.4724579128352078\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 305\n",
      "Training loss: 1.4747361161492087\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 306\n",
      "Training loss: 1.484536187215285\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 307\n",
      "Training loss: 1.4765014919367703\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 308\n",
      "Training loss: 1.4979808926582336\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 309\n",
      "Training loss: 1.4817912551489743\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 310\n",
      "Training loss: 1.4950382113456726\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 311\n",
      "Training loss: 1.4852127514102242\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 312\n",
      "Training loss: 1.4764111854813315\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 313\n",
      "Training loss: 1.4751412109895186\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 314\n",
      "Training loss: 1.5156714916229248\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 315\n",
      "Training loss: 1.4808571121909402\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 316\n",
      "Training loss: 1.4843419112942435\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 317\n",
      "Training loss: 1.5099246827038852\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 318\n",
      "Training loss: 1.508855397051031\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 319\n",
      "Training loss: 1.4875232902440159\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 320\n",
      "Training loss: 1.4702636220238425\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 321\n",
      "Training loss: 1.489049407568845\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 322\n",
      "Training loss: 1.4921622655608437\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 323\n",
      "Training loss: 1.4876014644449407\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 324\n",
      "Training loss: 1.4720112410458652\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 325\n",
      "Training loss: 1.4702954346483403\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 326\n",
      "Training loss: 1.492028908296065\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 327\n",
      "Training loss: 1.4654488942839883\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 328\n",
      "Training loss: 1.4957233450629495\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 329\n",
      "Training loss: 1.494792808185924\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 330\n",
      "Training loss: 1.4806709506294944\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 331\n",
      "Training loss: 1.4897990226745605\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 332\n",
      "Training loss: 1.5148879885673523\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 333\n",
      "Training loss: 1.4951070221987637\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 334\n",
      "Training loss: 1.472771167755127\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 335\n",
      "Training loss: 1.4673791365189985\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 336\n",
      "Training loss: 1.4695901274681091\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 337\n",
      "Training loss: 1.49403024261648\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 338\n",
      "Training loss: 1.51558425209739\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 339\n",
      "Training loss: 1.48233285817233\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 340\n",
      "Training loss: 1.4992684017528186\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 341\n",
      "Training loss: 1.4847522594711997\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 342\n",
      "Training loss: 1.5182798017155041\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 343\n",
      "Training loss: 1.4939084323969753\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 344\n",
      "Training loss: 1.4725673740560359\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 345\n",
      "Training loss: 1.5031332156874917\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 346\n",
      "Training loss: 1.483009154146368\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 347\n",
      "Training loss: 1.5128698023882778\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 348\n",
      "Training loss: 1.482216244394129\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 349\n",
      "Training loss: 1.489552075212652\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 350\n",
      "Training loss: 1.4719239419156855\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 351\n",
      "Training loss: 1.48317059061744\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 352\n",
      "Training loss: 1.4898984540592541\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 353\n",
      "Training loss: 1.522938229820945\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 354\n",
      "Training loss: 1.476457866755399\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 355\n",
      "Training loss: 1.4786725477738814\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 356\n",
      "Training loss: 1.482795233076269\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 357\n",
      "Training loss: 1.4895341396331787\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 358\n",
      "Training loss: 1.4838577183810147\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 359\n",
      "Training loss: 1.4937130863016301\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 360\n",
      "Training loss: 1.4836381619626826\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 361\n",
      "Training loss: 1.506213908845728\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 362\n",
      "Training loss: 1.4814839363098145\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 363\n",
      "Training loss: 1.4868244474584407\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 364\n",
      "Training loss: 1.4980883598327637\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 365\n",
      "Training loss: 1.520667937668887\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 366\n",
      "Training loss: 1.4914507378231396\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 367\n",
      "Training loss: 1.4922225962985645\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 368\n",
      "Training loss: 1.466757974841378\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 369\n",
      "Training loss: 1.489730639891191\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 370\n",
      "Training loss: 1.4879515387795188\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 371\n",
      "Training loss: 1.4699153683402322\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 372\n",
      "Training loss: 1.5069200667467983\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 373\n",
      "Training loss: 1.487373861399564\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 374\n",
      "Training loss: 1.4722257581624119\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 375\n",
      "Training loss: 1.4788057262247258\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 376\n",
      "Training loss: 1.4759867841547185\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 377\n",
      "Training loss: 1.4871630235151811\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 378\n",
      "Training loss: 1.4876580996946855\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 379\n",
      "Training loss: 1.499912435358221\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 380\n",
      "Training loss: 1.4739677906036377\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 381\n",
      "Training loss: 1.4838177453387866\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 382\n",
      "Training loss: 1.4728120619600469\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 383\n",
      "Training loss: 1.490905767137354\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 384\n",
      "Training loss: 1.478522468696941\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 385\n",
      "Training loss: 1.4807381413199685\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 386\n",
      "Training loss: 1.4777148365974426\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 387\n",
      "Training loss: 1.4725281541997737\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 388\n",
      "Training loss: 1.4747460917993025\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 389\n",
      "Training loss: 1.4989081350239841\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 390\n",
      "Training loss: 1.4791155078194358\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 391\n",
      "Training loss: 1.469646074555137\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 392\n",
      "Training loss: 1.5125090859153054\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 393\n",
      "Training loss: 1.4945138530297712\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 394\n",
      "Training loss: 1.4773126244544983\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 395\n",
      "Training loss: 1.5001154948364606\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 396\n",
      "Training loss: 1.5072922544045881\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 397\n",
      "Training loss: 1.4854100400751287\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 398\n",
      "Training loss: 1.4947667988863858\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 399\n",
      "Training loss: 1.5000770742242986\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 400\n",
      "Training loss: 1.4894292192025618\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 401\n",
      "Training loss: 1.5006065260280261\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 402\n",
      "Training loss: 1.4941038380969653\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 403\n",
      "Training loss: 1.4796766313639553\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 404\n",
      "Training loss: 1.491407118060372\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 405\n",
      "Training loss: 1.4750650037418713\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 406\n",
      "Training loss: 1.4970232031562112\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 407\n",
      "Training loss: 1.505415591326627\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 408\n",
      "Training loss: 1.5035962950099597\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 409\n",
      "Training loss: 1.4718561660159717\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 410\n",
      "Training loss: 1.5020269372246482\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 411\n",
      "Training loss: 1.5297412276268005\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 412\n",
      "Training loss: 1.4776179736310786\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 413\n",
      "Training loss: 1.4675614779645747\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 414\n",
      "Training loss: 1.4699821526354009\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 415\n",
      "Training loss: 1.4849105152216824\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 416\n",
      "Training loss: 1.4652575254440308\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 417\n",
      "Training loss: 1.4733294194394893\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 418\n",
      "Training loss: 1.4963709061796016\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 419\n",
      "Training loss: 1.4641084616834468\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 420\n",
      "Training loss: 1.4849876111203975\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 421\n",
      "Training loss: 1.4996321580626748\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 422\n",
      "Training loss: 1.5113993070342324\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 423\n",
      "Training loss: 1.482840435071425\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 424\n",
      "Training loss: 1.4952286861159585\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 425\n",
      "Training loss: 1.5197610259056091\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 426\n",
      "Training loss: 1.4671404795213179\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 427\n",
      "Training loss: 1.5260297710245305\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 428\n",
      "Training loss: 1.5126337734135715\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 429\n",
      "Training loss: 1.4747703888199546\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 430\n",
      "Training loss: 1.4802040349353442\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 431\n",
      "Training loss: 1.4773671952160923\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 432\n",
      "Training loss: 1.4708811207251116\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 433\n",
      "Training loss: 1.4892543120817705\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 434\n",
      "Training loss: 1.477300611409274\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 435\n",
      "Training loss: 1.4732298200780696\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 436\n",
      "Training loss: 1.5029043609445745\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 437\n",
      "Training loss: 1.4954102852127769\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 438\n",
      "Training loss: 1.4991971416906877\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 439\n",
      "Training loss: 1.4770860780369153\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 440\n",
      "Training loss: 1.471008539199829\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 441\n",
      "Training loss: 1.5007365291768855\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 442\n",
      "Training loss: 1.4796366962519558\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 443\n",
      "Training loss: 1.505322123115713\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 444\n",
      "Training loss: 1.504457565871152\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 445\n",
      "Training loss: 1.5150406902486628\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 446\n",
      "Training loss: 1.4919067458672957\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 447\n",
      "Training loss: 1.4802186543291265\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 448\n",
      "Training loss: 1.4943970116701992\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 449\n",
      "Training loss: 1.508680213581432\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 450\n",
      "Training loss: 1.506961470300501\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 451\n",
      "Training loss: 1.4833427450873635\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 452\n",
      "Training loss: 1.494184748692946\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 453\n",
      "Training loss: 1.4779052734375\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 454\n",
      "Training loss: 1.5007523406635632\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 455\n",
      "Training loss: 1.4890131787820295\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 456\n",
      "Training loss: 1.466129183769226\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 457\n",
      "Training loss: 1.5045513673262163\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 458\n",
      "Training loss: 1.4654340093786067\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 459\n",
      "Training loss: 1.5005514350804416\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 460\n",
      "Training loss: 1.4748588583686135\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 461\n",
      "Training loss: 1.5120745734734968\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 462\n",
      "Training loss: 1.4791730804876848\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 463\n",
      "Training loss: 1.501669244332747\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 464\n",
      "Training loss: 1.475934700532393\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 465\n",
      "Training loss: 1.5005757429383018\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 466\n",
      "Training loss: 1.4742481383410366\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 467\n",
      "Training loss: 1.474234250458804\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 468\n",
      "Training loss: 1.4707539948550137\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 469\n",
      "Training loss: 1.5009065541354092\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 470\n",
      "Training loss: 1.4855636791749434\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 471\n",
      "Training loss: 1.4995947480201721\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 472\n",
      "Training loss: 1.493789326060902\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 473\n",
      "Training loss: 1.4883910742673008\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 474\n",
      "Training loss: 1.4988309903578325\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 475\n",
      "Training loss: 1.4948212558572942\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 476\n",
      "Training loss: 1.4838538603349165\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 477\n",
      "Training loss: 1.4944483095949346\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 478\n",
      "Training loss: 1.4961692040616816\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 479\n",
      "Training loss: 1.4969907673922451\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 480\n",
      "Training loss: 1.4944124546918003\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 481\n",
      "Training loss: 1.4807850799777291\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 482\n",
      "Training loss: 1.482480683109977\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 483\n",
      "Training loss: 1.4681721383875066\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 484\n",
      "Training loss: 1.4844907251271335\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 485\n",
      "Training loss: 1.4853142337365584\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 486\n",
      "Training loss: 1.4941291023384442\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 487\n",
      "Training loss: 1.4785620949485085\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 488\n",
      "Training loss: 1.4756619876081294\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 489\n",
      "Training loss: 1.5108281970024109\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 490\n",
      "Training loss: 1.4693761522119695\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 491\n",
      "Training loss: 1.4725176312706687\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 492\n",
      "Training loss: 1.4836166880347512\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 493\n",
      "Training loss: 1.487019571391019\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 494\n",
      "Training loss: 1.4807550690390847\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 495\n",
      "Training loss: 1.4951655647971414\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 496\n",
      "Training loss: 1.481678382916884\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 497\n",
      "Training loss: 1.4989053877917202\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 498\n",
      "Training loss: 1.5002186081626199\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 499\n",
      "Training loss: 1.473435255614194\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 500\n",
      "Training loss: 1.5035091692751104\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 501\n",
      "Training loss: 1.4966907663778826\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 502\n",
      "Training loss: 1.4916892105882817\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 503\n",
      "Training loss: 1.4851942495866255\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 504\n",
      "Training loss: 1.5254494818774136\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 505\n",
      "Training loss: 1.4776431647214023\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 506\n",
      "Training loss: 1.4796503511342136\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 507\n",
      "Training loss: 1.4829136241566052\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 508\n",
      "Training loss: 1.4711306853727861\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 509\n",
      "Training loss: 1.4868027513677424\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 510\n",
      "Training loss: 1.480443298816681\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 511\n",
      "Training loss: 1.508270502090454\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 512\n",
      "Training loss: 1.4970995025201277\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 513\n",
      "Training loss: 1.4800691604614258\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 514\n",
      "Training loss: 1.4932620904662393\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 515\n",
      "Training loss: 1.4763092398643494\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 516\n",
      "Training loss: 1.5080396749756553\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 517\n",
      "Training loss: 1.4841195940971375\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 518\n",
      "Training loss: 1.4846267862753435\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 519\n",
      "Training loss: 1.4990925165739926\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 520\n",
      "Training loss: 1.473178348758004\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 521\n",
      "Training loss: 1.4848891469565304\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 522\n",
      "Training loss: 1.4709564284844832\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 523\n",
      "Training loss: 1.4898088303479282\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 524\n",
      "Training loss: 1.4827840301123532\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 525\n",
      "Training loss: 1.5033926421945745\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 526\n",
      "Training loss: 1.4816589084538547\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 527\n",
      "Training loss: 1.4826549237424678\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 528\n",
      "Training loss: 1.5031908371231772\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 529\n",
      "Training loss: 1.5166565667499194\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 530\n",
      "Training loss: 1.4831917773593555\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 531\n",
      "Training loss: 1.4755954525687478\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 532\n",
      "Training loss: 1.4788953987034885\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 533\n",
      "Training loss: 1.4733880378983237\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 534\n",
      "Training loss: 1.479267792268233\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 535\n",
      "Training loss: 1.4900735183195635\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 536\n",
      "Training loss: 1.4886377345431934\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 537\n",
      "Training loss: 1.4803063652732156\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 538\n",
      "Training loss: 1.479111297564073\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 539\n",
      "Training loss: 1.476820474321192\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 540\n",
      "Training loss: 1.4719982093030757\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 541\n",
      "Training loss: 1.5033920732411472\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 542\n",
      "Training loss: 1.4876952929930254\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 543\n",
      "Training loss: 1.500301176851446\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 544\n",
      "Training loss: 1.4843719384887002\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 545\n",
      "Training loss: 1.4942422834309665\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 546\n",
      "Training loss: 1.504131398417733\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 547\n",
      "Training loss: 1.467582876032049\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 548\n",
      "Training loss: 1.497378939932043\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 549\n",
      "Training loss: 1.4986525557257913\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 550\n",
      "Training loss: 1.4736495451493696\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 551\n",
      "Training loss: 1.4854305278171192\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 552\n",
      "Training loss: 1.4815655784173445\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 553\n",
      "Training loss: 1.4713993776928296\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 554\n",
      "Training loss: 1.485667651349848\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 555\n",
      "Training loss: 1.4859057448127053\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 556\n",
      "Training loss: 1.4812143607573076\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 557\n",
      "Training loss: 1.5033129020170732\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 558\n",
      "Training loss: 1.4718452312729575\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 559\n",
      "Training loss: 1.4726476723497564\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 560\n",
      "Training loss: 1.4885222803462634\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 561\n",
      "Training loss: 1.468617943200198\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 562\n",
      "Training loss: 1.4675796627998352\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 563\n",
      "Training loss: 1.4744283719496294\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 564\n",
      "Training loss: 1.5147220709107139\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 565\n",
      "Training loss: 1.480731481855566\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 566\n",
      "Training loss: 1.4749514216726476\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 567\n",
      "Training loss: 1.510574378750541\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 568\n",
      "Training loss: 1.4883451841094277\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 569\n",
      "Training loss: 1.4791631210934033\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 570\n",
      "Training loss: 1.4727051799947566\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 571\n",
      "Training loss: 1.5018777522173794\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 572\n",
      "Training loss: 1.4723743742162532\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 573\n",
      "Training loss: 1.472349925474687\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 574\n",
      "Training loss: 1.507909964431416\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 575\n",
      "Training loss: 1.4900160377675837\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 576\n",
      "Training loss: 1.524150626225905\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 577\n",
      "Training loss: 1.5014997774904424\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 578\n",
      "Training loss: 1.4850656986236572\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 579\n",
      "Training loss: 1.479867626320232\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 580\n",
      "Training loss: 1.5079585882750424\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 581\n",
      "Training loss: 1.470415630123832\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 582\n",
      "Training loss: 1.4854025136340747\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 583\n",
      "Training loss: 1.4842598058960654\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 584\n",
      "Training loss: 1.4697451970793984\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 585\n",
      "Training loss: 1.4806221940300681\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 586\n",
      "Training loss: 1.4987717433409258\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 587\n",
      "Training loss: 1.480314921249043\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 588\n",
      "Training loss: 1.4836025834083557\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 589\n",
      "Training loss: 1.526142873547294\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 590\n",
      "Training loss: 1.4783369194377551\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 591\n",
      "Training loss: 1.472878710790114\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 592\n",
      "Training loss: 1.5227347991683267\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 593\n",
      "Training loss: 1.4917476664889942\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 594\n",
      "Training loss: 1.511544325134971\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 595\n",
      "Training loss: 1.4913117560473355\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 596\n",
      "Training loss: 1.5167016278613696\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 597\n",
      "Training loss: 1.4734229066155173\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 598\n",
      "Training loss: 1.495647376233881\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 599\n",
      "Training loss: 1.4871306907046924\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 600\n",
      "Training loss: 1.496391469782049\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 601\n",
      "Training loss: 1.5028269507668235\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 602\n",
      "Training loss: 1.4827411825006658\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 603\n",
      "Training loss: 1.4719551693309436\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 604\n",
      "Training loss: 1.4729732843962582\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 605\n",
      "Training loss: 1.4836146885698491\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 606\n",
      "Training loss: 1.5278158025308088\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 607\n",
      "Training loss: 1.4878878864375027\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 608\n",
      "Training loss: 1.5132115212353794\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 609\n",
      "Training loss: 1.4721351320093328\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 610\n",
      "Training loss: 1.4817256710746072\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 611\n",
      "Training loss: 1.499575224789706\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 612\n",
      "Training loss: 1.4692123207178982\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 613\n",
      "Training loss: 1.4929982586340471\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 614\n",
      "Training loss: 1.4949996254660867\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 615\n",
      "Training loss: 1.4975957382809033\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 616\n",
      "Training loss: 1.4823131181977012\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 617\n",
      "Training loss: 1.504209117455916\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 618\n",
      "Training loss: 1.4738121141086926\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 619\n",
      "Training loss: 1.5088173747062683\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 620\n",
      "Training loss: 1.4706372618675232\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 621\n",
      "Training loss: 1.528568533333865\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 622\n",
      "Training loss: 1.4754331057721919\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 623\n",
      "Training loss: 1.4650292504917493\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 624\n",
      "Training loss: 1.5001792230389335\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 625\n",
      "Training loss: 1.4766190268776633\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 626\n",
      "Training loss: 1.4913135441866787\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 627\n",
      "Training loss: 1.4923069477081299\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 628\n",
      "Training loss: 1.505636990070343\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 629\n",
      "Training loss: 1.4971107948910107\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 630\n",
      "Training loss: 1.5039009072563865\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 631\n",
      "Training loss: 1.4761712713675066\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 632\n",
      "Training loss: 1.4729558717120776\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 633\n",
      "Training loss: 1.481776622208682\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 634\n",
      "Training loss: 1.5237055312503467\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 635\n",
      "Training loss: 1.5173941092057661\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 636\n",
      "Training loss: 1.4841709462079136\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 637\n",
      "Training loss: 1.4895355376330288\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 638\n",
      "Training loss: 1.4915685924616726\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 639\n",
      "Training loss: 1.4714467254551975\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 640\n",
      "Training loss: 1.4960484342141585\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 641\n",
      "Training loss: 1.4966307065703652\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 642\n",
      "Training loss: 1.4931285083293915\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 643\n",
      "Training loss: 1.4727123325521296\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 644\n",
      "Training loss: 1.4841274971311742\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 645\n",
      "Training loss: 1.47283014384183\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 646\n",
      "Training loss: 1.4965340115807273\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 647\n",
      "Training loss: 1.4873881990259343\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 648\n",
      "Training loss: 1.4705577167597683\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 649\n",
      "Training loss: 1.4868756478483027\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 650\n",
      "Training loss: 1.5313853946599094\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 651\n",
      "Training loss: 1.4915387088602239\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 652\n",
      "Training loss: 1.5017075430263171\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 653\n",
      "Training loss: 1.48946551843123\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 654\n",
      "Training loss: 1.474409742788835\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 655\n",
      "Training loss: 1.4971636642109265\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 656\n",
      "Training loss: 1.5154544711112976\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 657\n",
      "Training loss: 1.4957270947369663\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 658\n",
      "Training loss: 1.4969563538377935\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 659\n",
      "Training loss: 1.5160590247674421\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 660\n",
      "Training loss: 1.502504370429299\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 661\n",
      "Training loss: 1.4883001284165815\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 662\n",
      "Training loss: 1.491162825714458\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 663\n",
      "Training loss: 1.4938052242452449\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 664\n",
      "Training loss: 1.489333369515159\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 665\n",
      "Training loss: 1.4996758753603154\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 666\n",
      "Training loss: 1.4971610416065564\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 667\n",
      "Training loss: 1.4848600680177861\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 668\n",
      "Training loss: 1.4767300324006514\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 669\n",
      "Training loss: 1.4849190224300732\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 670\n",
      "Training loss: 1.4791206934235313\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 671\n",
      "Training loss: 1.4736242944544011\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 672\n",
      "Training loss: 1.49989162791859\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 673\n",
      "Training loss: 1.476663800803098\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 674\n",
      "Training loss: 1.4912868358872153\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 675\n",
      "Training loss: 1.4941900128668004\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 676\n",
      "Training loss: 1.469408094882965\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 677\n",
      "Training loss: 1.4734734432263807\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 678\n",
      "Training loss: 1.5084457451646978\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 679\n",
      "Training loss: 1.488462437282909\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 680\n",
      "Training loss: 1.4905505559661172\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 681\n",
      "Training loss: 1.4912234490567988\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 682\n",
      "Training loss: 1.5058201876553623\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 683\n",
      "Training loss: 1.5118272467093035\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 684\n",
      "Training loss: 1.5066325637427243\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 685\n",
      "Training loss: 1.5177910653027622\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 686\n",
      "Training loss: 1.4707347696477717\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 687\n",
      "Training loss: 1.4685710939494045\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 688\n",
      "Training loss: 1.473292502489957\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 689\n",
      "Training loss: 1.511511434208263\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 690\n",
      "Training loss: 1.503895954652266\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 691\n",
      "Training loss: 1.491322929208929\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 692\n",
      "Training loss: 1.4740094054828992\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 693\n",
      "Training loss: 1.4873938777230002\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 694\n",
      "Training loss: 1.4973604787479748\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 695\n",
      "Training loss: 1.4839400323954495\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 696\n",
      "Training loss: 1.47513516924598\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 697\n",
      "Training loss: 1.472363136031411\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 698\n",
      "Training loss: 1.50427307323976\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 699\n",
      "Training loss: 1.491661873730746\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 700\n",
      "Training loss: 1.5003140514547175\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 701\n",
      "Training loss: 1.474381918256933\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 702\n",
      "Training loss: 1.4946056387641213\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 703\n",
      "Training loss: 1.4741749113256282\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 704\n",
      "Training loss: 1.4712300354784185\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 705\n",
      "Training loss: 1.470671220259233\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 706\n",
      "Training loss: 1.4665511792356318\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 707\n",
      "Training loss: 1.4883324287154458\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 708\n",
      "Training loss: 1.4839650013230063\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 709\n",
      "Training loss: 1.4840648878704419\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 710\n",
      "Training loss: 1.4758446379141374\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 711\n",
      "Training loss: 1.4936530102383008\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 712\n",
      "Training loss: 1.4881967522881248\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 713\n",
      "Training loss: 1.5033722899176858\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 714\n",
      "Training loss: 1.475858834656802\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 715\n",
      "Training loss: 1.477790193124251\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 716\n",
      "Training loss: 1.4779433824799277\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 717\n",
      "Training loss: 1.482256217436357\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 718\n",
      "Training loss: 1.4780843583020298\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 719\n",
      "Training loss: 1.4857665408741345\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 720\n",
      "Training loss: 1.477663587440144\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 721\n",
      "Training loss: 1.482104561545632\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 722\n",
      "Training loss: 1.4911654374816201\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 723\n",
      "Training loss: 1.4910161061720415\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 724\n",
      "Training loss: 1.4922188520431519\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 725\n",
      "Training loss: 1.4714206782254307\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 726\n",
      "Training loss: 1.5275864005088806\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 727\n",
      "Training loss: 1.4751039418307217\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 728\n",
      "Training loss: 1.529666548425501\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 729\n",
      "Training loss: 1.486050313169306\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 730\n",
      "Training loss: 1.4967109723524614\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 731\n",
      "Training loss: 1.4807029312307185\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 732\n",
      "Training loss: 1.5111219828779048\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 733\n",
      "Training loss: 1.5043832551349292\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 734\n",
      "Training loss: 1.5022389672019265\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 735\n",
      "Training loss: 1.4917424266988581\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 736\n",
      "Training loss: 1.4655937552452087\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 737\n",
      "Training loss: 1.5049241239374334\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 738\n",
      "Training loss: 1.488737870346416\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 739\n",
      "Training loss: 1.4829414324326948\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 740\n",
      "Training loss: 1.481535624374043\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 741\n",
      "Training loss: 1.4955116402019153\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 742\n",
      "Training loss: 1.4828377691182224\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 743\n",
      "Training loss: 1.4870289103551344\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 744\n",
      "Training loss: 1.5052630467848345\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 745\n",
      "Training loss: 1.5091758912259883\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 746\n",
      "Training loss: 1.4988685250282288\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 747\n",
      "Training loss: 1.4795606082135981\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 748\n",
      "Training loss: 1.4817535281181335\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 749\n",
      "Training loss: 1.4733467156236821\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 750\n",
      "Training loss: 1.4989938735961914\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 751\n",
      "Training loss: 1.4948682134801692\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 752\n",
      "Training loss: 1.4954939484596252\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 753\n",
      "Training loss: 1.4819666689092463\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 754\n",
      "Training loss: 1.4843165711923079\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 755\n",
      "Training loss: 1.4950459816239097\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 756\n",
      "Training loss: 1.5089854489673267\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 757\n",
      "Training loss: 1.510559244589372\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 758\n",
      "Training loss: 1.4936512166803533\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 759\n",
      "Training loss: 1.4832073558460583\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 760\n",
      "Training loss: 1.4900086576288396\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 761\n",
      "Training loss: 1.4946691664782437\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 762\n",
      "Training loss: 1.5064453320069746\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 763\n",
      "Training loss: 1.4746230515566738\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 764\n",
      "Training loss: 1.4847514900294216\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 765\n",
      "Training loss: 1.4669164364988154\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 766\n",
      "Training loss: 1.4724014428528873\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 767\n",
      "Training loss: 1.4841318889097734\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 768\n",
      "Training loss: 1.4707410281354731\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 769\n",
      "Training loss: 1.4857249503785914\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 770\n",
      "Training loss: 1.5022677020593123\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 771\n",
      "Training loss: 1.4969452077692205\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 772\n",
      "Training loss: 1.5095430558378047\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 773\n",
      "Training loss: 1.4785947528752414\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 774\n",
      "Training loss: 1.4782143397764727\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 775\n",
      "Training loss: 1.4820789261297747\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 776\n",
      "Training loss: 1.4961755221540278\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 777\n",
      "Training loss: 1.5118482871489092\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 778\n",
      "Training loss: 1.4964531931010159\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 779\n",
      "Training loss: 1.5011920658024875\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 780\n",
      "Training loss: 1.4851479963822798\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 781\n",
      "Training loss: 1.4802442030473189\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 782\n",
      "Training loss: 1.4774216359311885\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 783\n",
      "Training loss: 1.4969118345867505\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 784\n",
      "Training loss: 1.5270779457959263\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 785\n",
      "Training loss: 1.4819555987011304\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 786\n",
      "Training loss: 1.5062013295563785\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 787\n",
      "Training loss: 1.5020853822881526\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 788\n",
      "Training loss: 1.4977527911012822\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 789\n",
      "Training loss: 1.4830521561882712\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 790\n",
      "Training loss: 1.4763475223021074\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 791\n",
      "Training loss: 1.5121975920417092\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 792\n",
      "Training loss: 1.475213180888783\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 793\n",
      "Training loss: 1.4864646521481601\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 794\n",
      "Training loss: 1.4859261512756348\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 795\n",
      "Training loss: 1.4847996830940247\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 796\n",
      "Training loss: 1.508061940019781\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 797\n",
      "Training loss: 1.495833239772103\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 798\n",
      "Training loss: 1.4767153750766406\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 799\n",
      "Training loss: 1.4732930714433843\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 800\n",
      "Training loss: 1.4962398409843445\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 801\n",
      "Training loss: 1.4743879600004717\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 802\n",
      "Training loss: 1.4776955192739314\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 803\n",
      "Training loss: 1.4791774153709412\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 804\n",
      "Training loss: 1.4672106070951982\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 805\n",
      "Training loss: 1.4886380488222295\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 806\n",
      "Training loss: 1.4759032346985557\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 807\n",
      "Training loss: 1.4874233386733315\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 808\n",
      "Training loss: 1.5046286582946777\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 809\n",
      "Training loss: 1.4800780686465176\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 810\n",
      "Training loss: 1.4710857217962092\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 811\n",
      "Training loss: 1.478523763743314\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 812\n",
      "Training loss: 1.4725180918520147\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 813\n",
      "Training loss: 1.474450875412334\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 814\n",
      "Training loss: 1.5089701847596602\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 815\n",
      "Training loss: 1.5153530673547224\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 816\n",
      "Training loss: 1.4749835946343162\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 817\n",
      "Training loss: 1.4855566620826721\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 818\n",
      "Training loss: 1.4852482297203757\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 819\n",
      "Training loss: 1.4882044412873008\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 820\n",
      "Training loss: 1.473012555729259\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 821\n",
      "Training loss: 1.4938792803070762\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 822\n",
      "Training loss: 1.5012700991197065\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 823\n",
      "Training loss: 1.489385572346774\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 824\n",
      "Training loss: 1.478874997659163\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 825\n",
      "Training loss: 1.4842554601756008\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 826\n",
      "Training loss: 1.4909154447642239\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 827\n",
      "Training loss: 1.490821740844033\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 828\n",
      "Training loss: 1.4941662820902737\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 829\n",
      "Training loss: 1.4858257445422085\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 830\n",
      "Training loss: 1.519149579785087\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 831\n",
      "Training loss: 1.4986940297213467\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 832\n",
      "Training loss: 1.4751889326355674\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 833\n",
      "Training loss: 1.4977536526593296\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 834\n",
      "Training loss: 1.5029555504972285\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 835\n",
      "Training loss: 1.4696198268370195\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 836\n",
      "Training loss: 1.4776513576507568\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 837\n",
      "Training loss: 1.5215110778808594\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 838\n",
      "Training loss: 1.479774702679027\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 839\n",
      "Training loss: 1.4747486331246116\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 840\n",
      "Training loss: 1.4951999946074053\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 841\n",
      "Training loss: 1.486441048708829\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 842\n",
      "Training loss: 1.5027029405940662\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 843\n",
      "Training loss: 1.4778586571866816\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 844\n",
      "Training loss: 1.4836673601107164\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 845\n",
      "Training loss: 1.4861448949033564\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 846\n",
      "Training loss: 1.4977591281587428\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 847\n",
      "Training loss: 1.4914810928431423\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 848\n",
      "Training loss: 1.4996924942189997\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 849\n",
      "Training loss: 1.488907440142198\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 850\n",
      "Training loss: 1.4896873073144392\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 851\n",
      "Training loss: 1.508106296712702\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 852\n",
      "Training loss: 1.47660692171617\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 853\n",
      "Training loss: 1.5133462927558206\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 854\n",
      "Training loss: 1.4820923751050776\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 855\n",
      "Training loss: 1.5083636208014055\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 856\n",
      "Training loss: 1.4827659780328923\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 857\n",
      "Training loss: 1.4867355986074968\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 858\n",
      "Training loss: 1.4743312814018943\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 859\n",
      "Training loss: 1.4700320688160984\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 860\n",
      "Training loss: 1.4799330938946118\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 861\n",
      "Training loss: 1.4825471585447139\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 862\n",
      "Training loss: 1.4670210805806247\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 863\n",
      "Training loss: 1.4729330810633572\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 864\n",
      "Training loss: 1.473756191405383\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 865\n",
      "Training loss: 1.4830747029998086\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 866\n",
      "Training loss: 1.5100391127846458\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 867\n",
      "Training loss: 1.5111987807533958\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 868\n",
      "Training loss: 1.4979339946400037\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 869\n",
      "Training loss: 1.5133214809677817\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 870\n",
      "Training loss: 1.486109581860629\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 871\n",
      "Training loss: 1.5006615736267783\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 872\n",
      "Training loss: 1.5024501139467412\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 873\n",
      "Training loss: 1.483128916133534\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 874\n",
      "Training loss: 1.4854149330746045\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 875\n",
      "Training loss: 1.4743222919377414\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 876\n",
      "Training loss: 1.4745987382802097\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 877\n",
      "Training loss: 1.4764022122729907\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 878\n",
      "Training loss: 1.4720211679285222\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 879\n",
      "Training loss: 1.469456965273077\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 880\n",
      "Training loss: 1.5040447820316662\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 881\n",
      "Training loss: 1.487021741541949\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 882\n",
      "Training loss: 1.496920108795166\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 883\n",
      "Training loss: 1.4918975775892085\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 884\n",
      "Training loss: 1.5111659548499368\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 885\n",
      "Training loss: 1.474291124127128\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 886\n",
      "Training loss: 1.5031602707776157\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 887\n",
      "Training loss: 1.4681784185496243\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 888\n",
      "Training loss: 1.4814186719330875\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 889\n",
      "Training loss: 1.5058813040906733\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 890\n",
      "Training loss: 1.5254303758794612\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 891\n",
      "Training loss: 1.499376492066817\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 892\n",
      "Training loss: 1.5008199269121343\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 893\n",
      "Training loss: 1.5119095065376975\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 894\n",
      "Training loss: 1.4727392684329639\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 895\n",
      "Training loss: 1.5104220184412869\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 896\n",
      "Training loss: 1.485014777291905\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 897\n",
      "Training loss: 1.5038816116072915\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 898\n",
      "Training loss: 1.474045542153445\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 899\n",
      "Training loss: 1.4740686145695774\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 900\n",
      "Training loss: 1.4993102225390347\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 901\n",
      "Training loss: 1.4914652434262363\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 902\n",
      "Training loss: 1.4777233546430415\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 903\n",
      "Training loss: 1.4799478650093079\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 904\n",
      "Training loss: 1.4715934937650508\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 905\n",
      "Training loss: 1.4776384180242366\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 906\n",
      "Training loss: 1.4831691763617776\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 907\n",
      "Training loss: 1.4815243157473477\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 908\n",
      "Training loss: 1.4728609702803872\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 909\n",
      "Training loss: 1.4862318526614795\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 910\n",
      "Training loss: 1.4729841134764932\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 911\n",
      "Training loss: 1.4677788669412786\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 912\n",
      "Training loss: 1.4804446751421148\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 913\n",
      "Training loss: 1.4913827505978672\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 914\n",
      "Training loss: 1.4970338994806462\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 915\n",
      "Training loss: 1.4783573313192888\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 916\n",
      "Training loss: 1.4714591611515393\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 917\n",
      "Training loss: 1.4738760482181201\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 918\n",
      "Training loss: 1.4863985505971042\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 919\n",
      "Training loss: 1.4941004352136091\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 920\n",
      "Training loss: 1.4923121387308294\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 921\n",
      "Training loss: 1.5099607868628069\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 922\n",
      "Training loss: 1.4920397238297896\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 923\n",
      "Training loss: 1.4923311092636802\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 924\n",
      "Training loss: 1.4879779057069258\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 925\n",
      "Training loss: 1.4911450852047314\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 926\n",
      "Training loss: 1.4860297712412747\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 927\n",
      "Training loss: 1.4811833067373796\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 928\n",
      "Training loss: 1.478008205240423\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 929\n",
      "Training loss: 1.4736286835236982\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 930\n",
      "Training loss: 1.471254446289756\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 931\n",
      "Training loss: 1.4706492857499556\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 932\n",
      "Training loss: 1.468058000911366\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 933\n",
      "Training loss: 1.4809410842982205\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 934\n",
      "Training loss: 1.497831555930051\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 935\n",
      "Training loss: 1.4784114306623286\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 936\n",
      "Training loss: 1.4901223941282793\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 937\n",
      "Training loss: 1.4983057325536555\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 938\n",
      "Training loss: 1.4877679944038391\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 939\n",
      "Training loss: 1.4838920777494258\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 940\n",
      "Training loss: 1.4789467183026401\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 941\n",
      "Training loss: 1.480206847190857\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 942\n",
      "Training loss: 1.4874151674183933\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 943\n",
      "Training loss: 1.4692484140396118\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 944\n",
      "Training loss: 1.5011524503881282\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 945\n",
      "Training loss: 1.4805425622246482\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 946\n",
      "Training loss: 1.4953591281717473\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 947\n",
      "Training loss: 1.4829913919622248\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 948\n",
      "Training loss: 1.4752395207231694\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 949\n",
      "Training loss: 1.4765046401457353\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 950\n",
      "Training loss: 1.5107815157283435\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 951\n",
      "Training loss: 1.4765320149334995\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 952\n",
      "Training loss: 1.504614764993841\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 953\n",
      "Training loss: 1.507114058191126\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 954\n",
      "Training loss: 1.4752203767949885\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 955\n",
      "Training loss: 1.4842432141304016\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 956\n",
      "Training loss: 1.482337306846272\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 957\n",
      "Training loss: 1.4800101627003064\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 958\n",
      "Training loss: 1.4978690201585942\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 959\n",
      "Training loss: 1.4939389824867249\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 960\n",
      "Training loss: 1.5039012730121613\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 961\n",
      "Training loss: 1.4734608422626148\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 962\n",
      "Training loss: 1.473465914076025\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 963\n",
      "Training loss: 1.472548246383667\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 964\n",
      "Training loss: 1.5030494765801863\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 965\n",
      "Training loss: 1.4935488538308577\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 966\n",
      "Training loss: 1.4875671104951338\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 967\n",
      "Training loss: 1.4965062195604497\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 968\n",
      "Training loss: 1.474832621487704\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 969\n",
      "Training loss: 1.519177409735593\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 970\n",
      "Training loss: 1.484727230938998\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 971\n",
      "Training loss: 1.469062314792113\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 972\n",
      "Training loss: 1.488100978461179\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 973\n",
      "Training loss: 1.5124787904999473\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 974\n",
      "Training loss: 1.5126222968101501\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 975\n",
      "Training loss: 1.484378538348458\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 976\n",
      "Training loss: 1.4891368584199385\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 977\n",
      "Training loss: 1.4819343090057373\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 978\n",
      "Training loss: 1.488976310599934\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 979\n",
      "Training loss: 1.5040036114779385\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 980\n",
      "Training loss: 1.4822513515299016\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 981\n",
      "Training loss: 1.4665199355645613\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 982\n",
      "Training loss: 1.5037554502487183\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 983\n",
      "Training loss: 1.478466586633162\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 984\n",
      "Training loss: 1.53664770451459\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 985\n",
      "Training loss: 1.505370096726851\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 986\n",
      "Training loss: 1.5097488381645896\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 987\n",
      "Training loss: 1.511726888743314\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 988\n",
      "Training loss: 1.4934288263320923\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 989\n",
      "Training loss: 1.4815593957901\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 990\n",
      "Training loss: 1.47906371680173\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 991\n",
      "Training loss: 1.5089383992281826\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 992\n",
      "Training loss: 1.4823468923568726\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 993\n",
      "Training loss: 1.4753358418291265\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 994\n",
      "Training loss: 1.4702266454696655\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 995\n",
      "Training loss: 1.5057555599646135\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 996\n",
      "Training loss: 1.4847699891437183\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 997\n",
      "Training loss: 1.4855878353118896\n",
      "Accuracy: 32.55813953488372%\n",
      "Epoch 998\n",
      "Training loss: 1.4814531803131104\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 999\n",
      "Training loss: 1.4889500357887961\n",
      "Accuracy: 25.58139534883721%\n",
      "Epoch 1000\n",
      "Training loss: 1.478974391113628\n",
      "Accuracy: 32.55813953488372%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):  #Running Train and Test\n",
    "    print(f'Epoch {epoch+1}')\n",
    "    train(model, loss, optimizer, train_loader)\n",
    "    test(model, test_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3: I selected a learning rate of .1 because this is pretty standard. I regularized my data in part 1 but in part 2 there was no need to since torch handled the values well. I used the Adam optimization algorithm. I used Adam to stablize the training process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dd44555f3ba73ce309ae5ab12787ae8bc32e7bf537ca4358849099f5ee68bb87"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
